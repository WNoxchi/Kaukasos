{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are my notes while debugging the issue on 4 & 5 June 2018\n",
    "\n",
    "Some formatting is lost in the conversion from Apple Notes to Markdown.\n",
    "\n",
    "WNixalo\n",
    "\n",
    "---\n",
    "\n",
    "### cifar10 codealong tensor issue\n",
    "\n",
    "## 1.\tTraining FastAI learner with PyTorch dataloaders:\n",
    "```\n",
    "TypeError: eq received an invalid combination of arguments - got (torch.LongTensor), but expected one of:\n",
    " * (int value)\n",
    "      didn't match because some of the arguments have invalid types: (torch.LongTensor)\n",
    " * (torch.cuda.LongTensor other)\n",
    "      didn't match because some of the arguments have invalid types: (torch.LongTensor)\n",
    "```\n",
    "\n",
    "`targs` in `fastai.metrics.accuracy` is `torch.LongTensor` \n",
    "- —> should be torch.cuda.LongTensor\n",
    "\n",
    "what is calling metrics.accuracy? where is `targs` created?\n",
    "- —> `fastai.model.validate` calls metrics as `f` on `preds.data` & `y`.\n",
    "\n",
    "that `y` is given to `model.validate`. where does `y` come from?\n",
    "- —> No. `model.validate` is given a dataloader dl, which it uses to get `y`.\n",
    "\n",
    "what’s the dataloader’s signature?\n",
    "- —> `<torch.utils.data.dataloader.DataLoader object at 0x7f5c9365b048>`\n",
    "- —> ```>>> vars(dl): {'dataset': <torchvision.datasets.folder.ImageFolder object at 0x7f5c949ce978>, 'batch_size': 512, 'num_workers': 4, 'collate_fn': <function default_collate at 0x7f5c96193268>, 'pin_memory': True, 'drop_last': False, 'timeout': 0, 'worker_init_fn': None, 'sampler': <torch.utils.data.sampler.SequentialSampler object at 0x7f5c9365b080>, 'batch_sampler': <torch.utils.data.sampler.BatchSampler object at 0x7f5c9365b0b8>}```\n",
    "\n",
    "let’s find where that dataloader was passed in from:\n",
    "- —> `fastai.model.fit` called `validate`.\n",
    "- —> `dl` is passed in as `cur_data.val_dl`\n",
    "\n",
    "my `val_loader` is not on the gpu.. should that be done manually? is any of my data on the gpu? \n",
    "- —> why is training working? what datatype is returned from my dataloaders?\n",
    "    - —> `next(iter(learner.data.val_dl))[0].type()` returns `‘torch.FloatTensor’`. As does …`trn_dl`….\n",
    "\n",
    "how do dataloaders work w/ the gpu? how does fastai do it?\n",
    "—> if I set up the usual fastai way, do I get cuda tensors?\n",
    "    - >>> tmfs=tfms_from_stats(stats,sz=32)\n",
    "    - >>> md = ImageClassifierData.from_csv(PATH, ‘train’, PATH/‘tmp.csv’, tfms=tfms)\n",
    "    - >>> next(iter(md.trn_dl))[0].type() returns ‘torch.cuda.FloatTensor`\n",
    "- —> ***my dataloaders aren’t working w/ the gpu***.\n",
    "\n",
    "what is the fastai mechanism responsible for putting dataloaders on the gpu?\n",
    "- —> `fastai.dataloader.DataLoader` class automatically sends data to the gpu IF available, ELSE cpu; via `.get_tensor(•)` via `fastai.core.to_gpu(•)`\n",
    "\n",
    "<<< PAUSE >>>\n",
    "\n",
    "---\n",
    "\n",
    "## 1A.\tUsing FastAI dataloaders:\n",
    "\n",
    "`TypeError: batch must contain numbers, dicts or lists; found <class 'torch.FloatTensor'>`\n",
    "\n",
    "why am I unable to pull data from the dataloader? how was it incorrectly initialized?\n",
    "- —> FastAI dls want me to play with `val_idxs`; I dont want to.\n",
    "- —> (pytorch non-cuda tensors) apparently [this is a pytorch bug](https://discuss.pytorch.org/t/dataloader-returning-non-cuda-tensors/13876). [Fixed by now](https://github.com/pytorch/pytorch/issues/5398#issuecomment-368313958). But I’m not using v0.4.\n",
    "- —> so can I just made a wrapper to call `to_gpu()` on w/e the pytorch dl returns?\n",
    "    - —> how the fuck do I do that.\n",
    "<<< END >>>\n",
    "\n",
    "---\n",
    "\n",
    "<<< RESUME >>>\n",
    "\n",
    "## 1B.\tTraining FastAI learner with PyTorch dataloaders:\n",
    "—> The Error:\n",
    "\n",
    "* via fastai ModelData&Loaders:\n",
    "`preds` is a `torch.autograd.variable.Variable`, but `preds.data` is a `torch.cuda.FloatTensor`. y is a **`torch.cuda.LongTensor`**.\n",
    "\n",
    "Calling `torch.max(X, dim=1)[1]` doesn’t change type for `X=preds`, but returns **`torch.cuda.LongTensor`** for `X=preds.data`.\n",
    "Comparing `f(preds.data)` & `y` is valid; `f(pred)`s & `y` is **not**.\n",
    "\n",
    "- `x`: `torch.cuda.FloatTensor`\n",
    "- `y`: **`torch.cuda.LongTensor`**\n",
    "    - `preds,loss = stepper(VV(x), VV(y))`\n",
    "- `preds: torch.autograd.variable.Variable`\n",
    "- `preds.data: `**`torch.cuda.FloatTensor`**\n",
    "- `type(torch.max(preds.data, dim=1)[1])`: **`torch.cuda.LongTensor`**\n",
    "\n",
    "\n",
    "* Now to see what’s different w/ pytorch dataloaders:\n",
    "\n",
    "x is a list of torch.FloatTensor. preds is torch.autograd.variable.Variable. preds.data is torch.cuda.FloatTensor. Where `f`:`torch.max(..)`: `f(preds)`:`torch.autograd.variable.Variable`; `f(preds.data,..)` yields **`torch.cuda.LongTensor`**. `y` is a **`torch.LongTensor`**. no `cuda`. Interesting. \n",
    "\n",
    "- `(x,y)` are yielded by `dl`, where `dl` is `cur_data.val_dl`\n",
    "- `x`: `torch.FloatTensor`\n",
    "- `y`: **`torch.LongTensor`**\n",
    "    - `preds,loss = stepper(VV(x), VV(y))`\n",
    "- `preds`: `torch.autograd.variable.Variable`\n",
    "- `preds.data`: **`torch.cuda.FloatTensor`**\n",
    "- `type(torch.max(preds.data, dim=1)[1])`: **`torch.cuda.LongTensor`**\n",
    "\n",
    "- —> this all points to the root of the problem being that `y` is yielded as a `torch.LongTensor` **instead of** a `torch.cuda.LongTensor` by the pytorch dataloader.\n",
    "\n",
    "\tjust found out everything ‘works fine’ if the batch size is 8. Currently finding the cut-off point. —> starts breaking at `bs=13`.\n",
    "\n",
    "---\n",
    "\n",
    "<< continuing >>\n",
    "\n",
    "`x` is automatically placed on GPU via `VV(x)`. `VV(•)` calls `map_over(•, VV_)` which calls `VV_` on every element of `•`. `VV_(•)` calls `create_variable(•, True)` which calls `Variable(T(•))`, and `T(•)` automatically puts `•` on the gpu via `to_gpu(•)`.\n",
    "\n",
    "using the version of fastai Radek used for his cifar10 baseline works…\n",
    "\n",
    "—>> The issue was different fastai versions. Somewhere there is an added/missing `to_gpu()` call on `y`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
