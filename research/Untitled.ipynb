{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization is a classic method to reudce over-fitting, and consists of adding \n",
    "# the weights of the model, multiplied by a given hyper-parameter (all eqns in this \n",
    "# article use python, numpy, and pytorch notation):\n",
    "final_loss = loss + wd * all_weights.pow(2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...where wd is the hyper-parameter to set. This is also called weight decay, because \n",
    "# when applying vanilla SGD it's equivalent to updating the weight like this:\n",
    "w = w - lr * w.grad - lr * wd * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the derivative of w^2 wrt w is 2w. In this eqn we see how we subtract a \n",
    "# little portion of the weight at each step, hence the name 'decay'.\n",
    "\n",
    "# Let's look at SGD with momentum for instance. Using L2 regularization consists of \n",
    "# adding wd*w to the gradients (as we saw earlier) but the gradients aren't \n",
    "# subtracted from the weights directly. First we compute a movign average:\n",
    "moving_avg = alpha * moving_avg + (1-alpha) * (w.grad + wd*w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...and it's this moving average that'll be multiplied by the leraning rate and \n",
    "# substracted from w. So the part linekd to the regularization that'll be taken from \n",
    "# w is lr*(1-alpha)*wd*w plus a combination of the previous weights that were already \n",
    "# in moving_avg.\n",
    "# \n",
    "# On the other hand, weight decay's update will look like:\n",
    "moving_avg = alpha * mpving_avg + (1-alpha) * w.grad\n",
    "w = w - lr*moving_avg - lr*wd*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_grads = beta1 * avg_grads + (1-beta1) * w.grad\n",
    "avge_squared = beta2 * (avg_squared) + (1-beta2) * (w.grad ** 2)\n",
    "w = w - lr * avg_grads / sqrt(avg_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amsgrad\n",
    "avg_grads = beta1 * avg_grads + (1-beta1) * w.grad\n",
    "avg_squared = beta2 * (avg_squared) + (1-beta2) * (w.grad **2)\n",
    "max_squared = max(avg_squared, max_squared)\n",
    "w = w - lr * avg_grads / sqrt(max_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (FastAI)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
